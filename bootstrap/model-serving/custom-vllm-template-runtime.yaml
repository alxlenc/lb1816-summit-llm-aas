apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    description: vLLM ServingRuntime running vllm 0.8.5
    internal.config.kubernetes.io/previousKinds: Template
    internal.config.kubernetes.io/previousNames: vllm-inferenceserver-cuda
    internal.config.kubernetes.io/previousNamespaces: opendatahub
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
    openshift.io/display-name: vLLM Inference Server
    openshift.io/provider-display-name: Red Hat, Inc.
    tags: rhods,rhoai,kserve,servingruntime
    template.openshift.io/documentation-url: https://github.com/opendatahub-io/vllm
    template.openshift.io/long-description: This template defines resources needed
      to deploy vLLM ServingRuntime with KServe in Red Hat OpenShift AI
  labels:
    app: odh-dashboard
    app.kubernetes.io/part-of: model-mesh
    app.opendatahub.io/model-mesh: "true"
    opendatahub.io/dashboard: "true"
    opendatahub.io/ootb: "false"
  name: vllm-inferenceserver-cuda
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    annotations:
      opendatahub.io/apiProtocol: REST
      opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
      openshift.io/display-name: "vLLM rhoai-2.19-cuda"
      opendatahub.io/template-display-name: "vLLM rhoai-2.19-cuda"
      opendatahub.io/template-name: vllm-2.16-6144
    name: vllm-cuda
    namespace: llm-hosting
    labels:
      component: aiworkshop
      opendatahub.io/dashboard: 'true'
      rht-gitops.com/openshift-gitops: model-serving
  spec:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: '8080'
    containers:
      - args:
          - --port=8080
          - --model=/mnt/models
          - --served-model-name={{.Name}}
          - --distributed-executor-backend=mp
        command:
          - python
          - '-m'
          - vllm.entrypoints.openai.api_server
        env:
          - name: HF_HOME
            value: /tmp/hf_home
        image: 'quay.io/modh/vllm:rhoai-2.19-cuda'
        name: kserve-container
        ports:
          - containerPort: 8080
            protocol: TCP
        volumeMounts:
          - mountPath: /dev/shm
            name: shm
    multiModel: false
    supportedModelFormats:
      - autoSelect: true
        name: vLLM
    volumes:
      - emptyDir:
          medium: Memory
          sizeLimit: 2Gi
        name: shm
